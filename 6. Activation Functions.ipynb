{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0f01d2",
   "metadata": {},
   "source": [
    "# <span style = \"color:DarkOrchid\"> What is a Neural Network Activation Function? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbef216",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4d779",
   "metadata": {},
   "source": [
    "<b>An Activation Function</b> decides whether a neuron should be activated or not. This means that it decides whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations.\n",
    "\n",
    "The role of the Activation funciton is to derive output from a set of input values fed to a node (or a layer).\n",
    "\n",
    "But---\n",
    "\n",
    "What exactly is a node?\n",
    "\n",
    "If we compare the neural network to our brain, a node is a replica of a neuron that receives a set of input signals-external stimuli.\n",
    "\n",
    "Depending on the nature and intensity of these input signals, the brain processes them and decides whether the neuron should be activated(\"fired\") or not.\n",
    "\n",
    "In deep learning, this is also the role of the Activation Function--that's why it's often referred to as a Transfer Function in Artificial Neural Network.\n",
    "\n",
    "The primary role of the Activation Function is to transform the summed weighted input from the node into an output value to be fed to the next hidden layer or as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0494c",
   "metadata": {},
   "source": [
    "![title](2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0b92f",
   "metadata": {},
   "source": [
    "## Why do Neural Networks need an Activation Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d51edd",
   "metadata": {},
   "source": [
    "So we know what Activation Function is and what it does, but-- \n",
    "\n",
    "Why do Neural Networks need it?\n",
    "\n",
    "Well, the purpose of an activation function is to add non-linearity to the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbcc564",
   "metadata": {},
   "source": [
    "![title](3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cec4a8",
   "metadata": {},
   "source": [
    "Activation function introduce an additional step at each layer during the forward propogation, but its computation is worth it. Here is why--\n",
    "\n",
    "Let's suppose we have a neural network working without the activation functions.\n",
    "\n",
    "In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. It's because it doesn't matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself.\n",
    "\n",
    "Although the neural network becomes simpler, learning any complex task is impossible, and our model would be just a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dcb870",
   "metadata": {},
   "source": [
    "## Different Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5df3b",
   "metadata": {},
   "source": [
    "#### 1. Sigmoid / Logistic Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a06ce5",
   "metadata": {},
   "source": [
    "This function takes any real value as input and outputs values in the range of 0 to 1. \n",
    "\n",
    "The larger the input(more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357a525",
   "metadata": {},
   "source": [
    "![title](4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa393b",
   "metadata": {},
   "source": [
    "Mathematically it can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ffcf2",
   "metadata": {},
   "source": [
    "![title](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8db05",
   "metadata": {},
   "source": [
    "#### 2.Tanh Function (Hyperbolic Tangent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2fba1",
   "metadata": {},
   "source": [
    "Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6065907",
   "metadata": {},
   "source": [
    "![title](6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e420ea",
   "metadata": {},
   "source": [
    "Mathematically it can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848db8b",
   "metadata": {},
   "source": [
    "![title](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09edaf",
   "metadata": {},
   "source": [
    "#### 3. ReLU Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b00c0",
   "metadata": {},
   "source": [
    "ReLu stands for Rectified Linear Unit.\n",
    "\n",
    "Although it gives an impression of linear funciton, ReLu has a derivative function and allows backpropogation while simultaneously making it computationally efficient.\n",
    "\n",
    "The main catch here is that ReLu function does not activate all the neurons at the same time.\n",
    "\n",
    "The neurons will only be deactivated if the output of the linear transformation is less than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e2ee1",
   "metadata": {},
   "source": [
    "![title](7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ca8c0",
   "metadata": {},
   "source": [
    "Mathematically it can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7461892",
   "metadata": {},
   "source": [
    "![title](8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce184e24",
   "metadata": {},
   "source": [
    "#### 4. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fb1c4",
   "metadata": {},
   "source": [
    "Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce47c6",
   "metadata": {},
   "source": [
    "![title](8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c646c",
   "metadata": {},
   "source": [
    "Mathematically it can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23214235",
   "metadata": {},
   "source": [
    "![title](9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c8e87",
   "metadata": {},
   "source": [
    "#### 5. Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abc276",
   "metadata": {},
   "source": [
    "Before exploring the ins and outs of the Softmax activation funciton, we should focus on its building block-the sigmoid/logistic activation function that works on calculating probability values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54d0d3",
   "metadata": {},
   "source": [
    "![title](10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99953135",
   "metadata": {},
   "source": [
    "The output of the sigmoid function was in range of 0 to 1, which can be thought of as probabilty. \n",
    "\n",
    "But-- \n",
    "\n",
    "This function faces certain problems.\n",
    "\n",
    "Let's suppose we have five output values of 0.8, 0.9, 0.7, 0.8, and 0.6 respectively. How can we move forward with it?\n",
    "\n",
    "The answer is: We can't.\n",
    "\n",
    "The above values don't make sense as the sum of all the classes/output probabilities should be equal to 1.\n",
    "\n",
    "You see, the softmax function is described as a combination of multiple sigmoids.\n",
    "\n",
    "It calculates the relative probabilities. Similar to the sigmoid/logistic activation function, the softmax function returns the probability of each class.\n",
    "\n",
    "It is most commonly used as an activation function for the last layer of the neural network in the case of multi-class classification.\n",
    "\n",
    "Mathematically it can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece7a83",
   "metadata": {},
   "source": [
    "![title](11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143db48e",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
