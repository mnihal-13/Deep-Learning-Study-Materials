{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e43ba5",
   "metadata": {},
   "source": [
    "# <span style = \"color:marble\"> Different RNN Architectures</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28bc09",
   "metadata": {},
   "source": [
    "There are different variations of RNNs that are being applied practically in machine learning problems:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd3ffa",
   "metadata": {},
   "source": [
    "### 1. Bidirectional recurrent neural networks (BRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303feb23",
   "metadata": {},
   "source": [
    "These are a varient network architecture of RNNs. While unidirectional RNNs can only drawn from previous inputs to make predictions about the current state, bidirectional RNNs pull in future data to improve the accuracy of it. If we return to the example of \"feeling under the weather\" earlier in this article, the model can better predict that the second word in that phrase is \"under\" if it knew that the last word in the sequence is \"weather\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297a2bf",
   "metadata": {},
   "source": [
    "### 2. Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c7d12",
   "metadata": {},
   "source": [
    "This is a popular RNN architecture, which was introduced by Sepp Hochreiter and Juergen Schmidhuber as a solution to vanishing gradient problem. In their paper, they work to address the problem of long-term dependencies. That is, if the previous state that is influencing the current prediction is not in the recent past, the RNN model may not be able to accurately predict the current state. As an example, let's say we wanted to predict the italicized words in following, \"Alice is allergic to nuts. She can't eat peanut butter\". The context of a nut allergy can help us anticipate that the food that cannot be eaten contains nuts. However, if that context was a few sentences prior, then it would make it difficult, or even impossible, for the RNN to connect the information. To remedy this, LSTMs have \"cell\" in the hidden layers of the neural network, which have three gates-an input gate, an output gate, and a forget gate. These gates control the flow of information which is needed to predict the output in the network. For example, if gender pronouns, such as \"she\", was repeated multiple times in prior sentences, you may exclude that from the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266cfb4",
   "metadata": {},
   "source": [
    "### 3. Gated Recurrent units(GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bceb4",
   "metadata": {},
   "source": [
    "This RNN varient is similar to the LSTMs as it also works to address the short term memory problem of RNN models. Instead of using a \"Cell state\" regulate information, it uses hidden states, and instead of  three gates, it has two-a reset gate. Similar to the gates within LSTMs, the reset and update gates control how much and which information to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0609f",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
